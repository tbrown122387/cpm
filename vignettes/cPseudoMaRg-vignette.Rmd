---
title: "Using cPseudoMaRg"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{cPseudoMaRg-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: bibliography.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


## What does this package provide?

This package provides the boilerplate code for the correlated pseudo-marginal method of [@cpm]. This vignette demonstrates its use by providing two examples that are closely related to ones given in the above paper. 

## What do I need to know to use this software?

Before getting to the examples, however, I provide a more detailed description of this package's primary function: `makeCPMSampler()`. This is a *function factory* in that it is function that creates and returns a function. Many of `makeCPMSampler()`'s required arguments are functions, as well. 

To use this package, you will have to be somewhat familiar with Metropolis-Hasting's-type algorithms. Here is an extremely short description of how these work. 

Assume that you are a Bayesian and have some data $y$, and that you would like to perform inference for a collection of parameters $\theta$. After specifying a likelihood $p(y \mid \theta)$ and a prior $p(\theta)$, you are interested in sampling from the posterior $p(\theta \mid y)$ using a Metropolis-Hastings-type algorithm. Such an algorithm will draw correlated samples $\theta^1, \theta^2, \ldots, \theta^{N_{S}}$ from a Markov chain, and under suitable regularity conditions, the average of your large collection of samples will approximate posterior expectations such as the posterior mean: $E[\theta \mid y]$.

Metropolis-Hastings type algorithms will generate the Markov sequence in a clever way. But you still need to provide a few things. One of the things you need to provide is a (well-tuned) proposal transition distribution. This is what proposes new parameters from old. Some of these proposals will be accepted, and others won't. You will need to be able to sample from this function, and to evaluate it. You provide it to `logLikeApproxEval()` in the first two arguments: `paramKernSamp` and `logParamKernEval`.

Recall that your specified model has a likelihood and a prior. You will need to provide a function that evaluates the (log of the) prior. This is the third argument: `logPriorEval`. 

Unlike the standard Metropolis-Hastings algorithm, you will not be required to provide a function that evaluates the likelihood. Recall that this function provides a  pseudo-marginal sampler, TODO cite, so you are only required to give an unbiased estimate of the likelihood. This is provided as the fourth argument: `logLikeApproxEval`.^[If you'd like, you may provide for this argument a function giving exact evaluations. In this case, `makeCPMSampler()` will return a standard MH sampler. This can be useful if you are comparing the relative efficiency of pseudo-marginal and non-pseudo-marginal methods.]

Finally, you must provide the non-functional arguments. `yData` is the entire data set of observations. `numU` is the number of standard univariate normal samples that are used for each approximate log-likelihood evaluation. `numIters` is the total number of samples drawn. This does not count burn-in or thinning. `rho` is the correlation between standard normal variates at each MCMC iteration. Finally, change `storeEvery` to some integer greater than $1$ if you are concerned about memory on your computer, and you'd like to use thinning. If you set it to $2$, say, then every other sample will be retained.


## A First Example

The first example is the same as the provided in TODO's documentation. This section breaks up the code into chunks in order to explain it more easily. The model has two parameters: $\theta = (\theta_1, \theta_2)$. The conditional likelihood is


$$
\begin{eqnarray} 
p(y_{1:N} \mid x_{1:N}, \theta) = \prod_{i=1}^N p(y_{i} \mid x_{i}, \theta)  \tag{1}
\end{eqnarray}
$$

where
$$
\begin{eqnarray} 
y_{i} \mid x_{i}, \theta \sim \text{Normal}(x_i, \theta_1 - \theta_2).
\end{eqnarray}
$$
Here $x_1, \ldots, x_N := x_{1:N}$ is a collection of latent/hidden/unobserved random variables. They have the following distribution:

$$
\begin{eqnarray} 
p(x_{1:N} \mid \theta) = \prod_{i=1}^N p( x_{i} \mid \theta) \tag{2}
\end{eqnarray}
$$

where

$$
\begin{eqnarray} 
x_{i} \mid  \theta \sim \text{Normal}(x_i, \theta_2)
\end{eqnarray}.
$$

We can simulate $10$ observations with the following code. 

```{r, collapse = TRUE, echo=FALSE}
realTheta1 <- .2 + .3
realTheta2 <- .2
realParams <- c(realTheta1, realTheta2)
numObs <- 10
realX <- rnorm(numObs, mean = 0, sd = sqrt(realTheta2))
realY <- rnorm(numObs, mean = realX, sd = sqrt(realTheta1 - realTheta2))
```

Next, construct the sampler function using `makeCPMSampler()`.

```{r, collapse=TRUE}
library(cPseudoMaRg)
numImportanceSamps <- 1000
numMCMCIters <- 1000
randomWalkScale <- 1.5
recordEveryTh <- 1
sampler <- makeCPMSampler(
  paramKernSamp = function(params){
    return(params + rnorm(2)*randomWalkScale)
  },
  logParamKernEval = function(oldTheta, newTheta){
    dnorm(newTheta[1], oldTheta[1], sd = randomWalkScale, log = TRUE)
    + dnorm(newTheta[2], oldTheta[2], sd = randomWalkScale, log = TRUE)
  },
  logPriorEval = function(theta){
    if( (theta[1] > theta[2]) & all(theta > 0)){
      0
    }else{
      -Inf
    }
  },
  logLikeApproxEval = function(y, thetaProposal, uProposal){
    if( (thetaProposal[1] > thetaProposal[2]) & (all(thetaProposal > 0))){
      xSamps <- uProposal*sqrt(thetaProposal[2])
      logCondLikes <- sapply(xSamps,
                             function(xsamp) {
                               sum(dnorm(y,
                                         xsamp, 
                                         sqrt(thetaProposal[1] - thetaProposal[2]),
                                         log = TRUE)) })
      m <- max(logCondLikes)
      log(sum(exp(logCondLikes - m))) + m - log(length(y))
    }else{
      -Inf
    }
  },
  realY, 
  numImportanceSamps, 
  numMCMCIters, 
  .99, # change to 0 for original pseudo-marginal method
  recordEveryTh)
```

The approximate log-likelihood, provided to `logLikeApproxEval`, uses importance sampling (TODO cite). 

$$
\begin{align*} 
\log \hat{p}(y_{1:N} \mid  \theta) 
&= \log \left\{ 
N^{-1} \sum_{i=1}^N \frac{p(y_{1:N} \mid x_{1:N}, \theta) p(x_{1:N} \mid \theta)}{q(x_{1:N} \mid y_{1:N}, \theta)}
\right\}
\end{align*}
$$
We choose $q(x_{1:N} \mid y_{1:N}, \theta) = p(x_{1:N} \mid \theta)$, despite it not being the optimal proposal/instrumental distribution. Note the use of the "log-sum-exp" trick to avoid numerical underflow TODO cite.

Finally, sample the Markov chain. Call the samples `res`, and examine them. 

```{r, collapse=TRUE, out.width='80%',}
res <- sampler(realParams)
print(res)
plot(res)
```

This particular model has a tractable likelihood, so it is more efficient to use regular MH. It is equal to the following

$$
\begin{align*} 
p(y_{1:N} \mid  \theta) 
&= \int p(y_{1:N} \mid x_{1:N}, \theta) p(x_{1:N} \mid \theta) \text{d}x_{1:N} \\
&= \prod_{i=1}^N \text{Normal}(y_i ; 0, \theta_1)
\end{align*}
$$


```{r, collapse=TRUE, out.width='80%'}
samplerExact <- makeCPMSampler(
  paramKernSamp = function(params){
    return(params + rnorm(2)*randomWalkScale)
  },
  logParamKernEval = function(oldTheta, newTheta){
    dnorm(newTheta[1], oldTheta[1], sd = randomWalkScale, log = TRUE)
    + dnorm(newTheta[2], oldTheta[2], sd = randomWalkScale, log = TRUE)
  },
  logPriorEval = function(theta){
    if( (theta[1] > theta[2]) & all(theta > 0)){
      0
    }else{
      -Inf
    }
  },
  logLikeApproxEval = function(y, thetaProposal, uProposal){
    # this is exact now!
    if( (thetaProposal[1] > thetaProposal[2]) & (all(thetaProposal > 0))){
      sum(dnorm(y, mean = 0, sd = sqrt(thetaProposal[1]), log = TRUE))
    }else{
      -Inf
    }
  },
  realY, 
  numImportanceSamps, # doesn't this matter because Us are not used
  numMCMCIters, 
  .99, # doesn't this matter because Us are not used
  recordEveryTh)
res2 <- samplerExact(realParams)
print(res2)
plot(res2)
```

## A Second Example

Consider the following stochastic volatility [@taylor_svol]. It assumes a latent AR(1) process. It also assumes that, after conditioning on contemporaneous values of the latent process, that the observed returns are normal. 

In other words, let $Z_1, \ldots, Z_N$ and $W_1, \ldots, W_N$ be iid standard Normal random variables, and for $t > 1$, define the latent process as

$$
X_t = \phi X_{t-1} + \sigma Z_t,
$$
and $X_1 \sim \text{Normal}(0, \sigma^2/(1-\phi^2))$. The distribution of the observed returns is defined by
$$
Y_t = \beta  \exp(X_t/2) W_t
$$
for $t = 1, \ldots, N$. The collection of all unknown parameters is $\theta = \phi, \beta, \sigma^2$.


$Y_{1:N} \mid X_{1:N}, \theta$ has the same factorization as equation (1), but unlike equation (2), we have the following Markovian factorization for the distribution of all latent variables:

$$
\begin{eqnarray} 
p(x_{1:N} \mid \theta) = p(x_1) \prod_{t=2}^N p( x_{t} \mid x_{t-1}, \theta) \tag{3}.
\end{eqnarray}
$$
We assume that all three parameters are independent a prior: $\pi(\phi)\pi(\beta)\pi(\sigma^2)$, and we select a uniform, Gaussian, and Inverse-Gamma distribution for these three distributions, respectively. 

The primary bottleneck in the previous example was the function passed in to the `logLikeApproxEval=` argument of `makeCPMSampler()`. It was written entirely in R, and so, as a result, was not as fast as it could have been. In this example, we provide a compiled function to this argument that makes use of C++ code taken from the PF C++ library [@Brown2020]. Interfacing to this from R was facilitated by the RcppEigen package [@rcppeigen]. A growing collection of examples of calling PF code from R is provided as an RcppEigen R package called `pfexamplesinr` that is hosted on Github. 

The particle filter used to compute the approximate log-likelihood is described in [@cpm]. It uses common random numbers, and a resampling technique that makes of the pseudo-inverse of a Hilbert space-filling function. This function, called $h$ in [@cpm], was implemented with C++ code lightly edited from the C code provided in [@hilbert]. 

Version 1.0.1 of this package provides an extra argument to `makeCPMSampler()`: `nansInLLFatal=`. If set to `FALSE`, then, whenever `NaN` is returned from the particle filter's approximate log-likelihood function, the parameter proposal is disregarded. If set to `TRUE`, then the entire chain comes to a halt. I recommendset this argument to `FALSE`

Please also note that the number of particles used in this particle filter is selected in two places. One place is the code below, and the other is in a c++ `#define` directive in the file `src/likelihoods.cpp` in the `pfexamples` library. 


- show smoothness of resampling mechanism...profile log-likelihoods with common random numbers
- particle filters can produce NaNs in the log-like eval, especially if your parameter proposal scale is huge

```{r, collapse=TRUE, out.width='80%'}
library(cPseudoMaRg)
devtools::install_github("tbrown122387/pfexamplesinr@e4e2a80")
library(pfexamplesinr)

returnsData <- read.csv("data/return_data.csv", header=F)[,1]
numParticles <- 500 # THIS MUST MATCH "#define NP 500" in src/likelihoods.cpp
numMCMCIters <- 100
randomWalkScale <- .1
recordEveryTh <- 1
numUs <- length(returnsData)*(numParticles+1)

# some helper functions
transformParams <- function(untrans){
  p <- vector(mode = "numeric", length = 3)
  p[1] <- boot::logit(untrans[1])
  p[2] <- untrans[2]
  p[3] <- log(untrans[3])
  return(p)
}
revTransformParams <- function(trans){
  p <- vector(mode = "numeric", length = 3)
  p[1] <- boot::inv.logit(trans[1])
  p[2] <- trans[2]
  p[3] <- exp(trans[3])
  return(p)
}

sampler <- makeCPMSampler(
  paramKernSamp = function(params){
    revTransformParams(transformParams(params) + rnorm(3)*randomWalkScale)
  },
  logParamKernEval = function(oldTheta, newTheta){
    dnorm(newTheta[1], oldTheta[1], sd = randomWalkScale, log = TRUE)
    + dnorm(newTheta[2], oldTheta[2], sd = randomWalkScale, log = TRUE) # TODO
  },
  logPriorEval = function(theta){
    if( (abs(theta[1]) >= 1.0) || theta[3] <= 0.0 ){
      -Inf
    }else{
       log(.5) + 
        dnorm(theta[2], mean = 0, sd = 10, log = T) + 
        dgamma(x = 1/theta[3], shape = 1.3, rate = .3, log = T)
    }
  },
  logLikeApproxEval = svolApproxLL, 
  returnsData, numUs, numMCMCIters, .99, recordEveryTh#, FALSE
)

svolSampleResults <- sampler( c(.9, 1, .1))
mean(svolSampleResults)
print(svolSampleResults)
```

# References
